
"use strict";

module.exports = {
  /*
   * The URI of the hub command api.
   */
  "commandHost": "http://localhost:3102",

  /*
   * The target dataset in the hub.
   */
  "targetDataset": {
    "id": "dataset-3EHDx6",
    "scheme": {
        "accidentIndex": "String",
        "vehicleRef": "Number",
        "vehicleType": "Number",
        "towing": "Date",
        "manoeuvre": "Date",
        "restrictedLane": "Number",
        "junction": "Number",
        "skidding": "Number",
        "objectIn": "Number",
        "leavingRoad": "Number",
        "objectOff": "Number",
        "firstImpact": "Number",
        "lhd": "Number",
        "journeyPurpose": "Number",
        "sex": "Number",
        "ageBand": "Number",
        "engineCC": "Number",
        "propulsion": "Number",
        "vehicleAge": "Number",
        "driverDecile": "Number",
        "driverHomeArea": "Number"
    }
  },

  /*
   * Define how the CSV columns will map to the schema defined in the dataset.
   * The schemaMapping array will reflect the order of the columns in the CSV.
   * If a target is defined for a column the data will be copied to the named field in the dataset.
   * If there is no target property that column will be ignored.
   */
  "schemaMapping": [
    {target: "accidentIndex"},
    {target: "vehicleRef"},
    {target: "vehicleType"},
    {target: "towing"},
    {target: "manoeuvre"},
    {target: "restrictedLane"},
    {target: "junction"},
    {target: "skidding"},
    {target: "objectIn"},
    {target: "leavingRoad"},
    {target: "objectOff"},
    {target: "firstImpact"},
    {target: "lhd"},
    {target: "journeyPurpose"},
    {target: "sex"},
    {target: "ageBand"},
    {target: "engineCC"},
    {target: "propulsion"},
    {target: "vehicleAge"},
    {target: "driverDecile"},
    {target: "driverHomeArea"}
  ],

  /*
   * The location of the source file.
   */
  "sourceUrl": "http://two268.com/nqm/Vehicles0513.csv",

  /*
   * The processing mode - need to experiment with which is more performant, but there may be some cases
   * where it's desirable to download the file and store it locally before processing.
   * Possible values are:
   *
   * local => file is downloaded and then the entire contents are read into memory at once, parsed and then processed.
   * Probably the fastest but not suitable for larger files.
   *
   * localStream => file is downloaded and then streamed into the parser and processed as data becomes available.
   * Suitable for larger files.
   *
   * remoteStream => file is not downloaded but streamed directly from the remote url into the parser and processed
   * as data becomes available.
   */
  "processingMode": "localStream",

  /*
   * The delimiter used in the source file.
   */
  "delimiter": ",",

  /*
   * Expected encoding of source data.
   */
  "encoding": "utf8",

  /*
   * The line at which processing is to start.
   */
  "startLine": 1,

  /*
   * The line at which processing is to stop.
   * Specify -1 for the entire file.
   */
  "endLine": -1,

  /*
   * Specifies how to throttle the input stream so that the parser doesn't get overwhelmed.
   * Use this to
   * Value is bytes/second.
   */
  "throttleRate": 50000,

  "bulkMode": true
};
